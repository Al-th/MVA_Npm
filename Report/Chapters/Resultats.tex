\section{Résultats}
Afin d'analyser les performances de la méthode proposée par \cite{bib_gicp}, nous avons tout d'abord cherché à étudier l'influence du critère $d_{max}$ qui rentre en compte lors de l'appariement des points. Dans un second temps nous avons voulu évaluer la robustesse de l'algorithme face à différentes transformations initiales et enfin la différence de résultats entre l' ICP standard obtenu avec la formule analytique et l'ICP standard obtenu avec le modèle probabiliste présenté dans l'article (en posant $C_{i}^B=Id$ et $C_{i}^A=0$).\\

Étant donné que GICP utilise l'information des surfaces de chacun des nuages de points, l'algorithme présente théoriquement une meilleure robustesse à la différence d'échantillonnage entre les deux nuages de points. Ainsi, l'ensemble des tests seront réalisés à la fois sur une paire de scans synthétiques présentée dans la figure \ref{fig_nuage_bunny} et une paire de scans réels acquise par l'université de Hannovre présentée dans la figure \ref{fig_nuage_recalage} . On dispose pour les scans réels de l'estimation de leur poses, i.e une matrice de transformation $\hat{\mathbf{T}}$ que nous pouvons utiliser comme vérité terrain. Enfin, pour générer la paire de scans synthétiques, une transformation connue $T^{*}$ est appliquée sur le nuage de points synthétiques puis la position des points du nuage transformé est modifié par un bruit gaussien.\\

La transformation initiale $\mathbf{T}_0$ est définie comme la transformation exacte $\mathbf{T}^*$ auquel on ajoute une erreur en translation ($\pm2m$ pour la base Hannovre, $\pm0.1cm$ pour \textit{bunny}) et en rotation ($\pm15^{\circ}$ selon tous les axes). Afin d'évaluer les performances, on mesure l'erreur de position moyenne $||B - \mathbf{T}(A)||_2$ %TODO (expliquer comme on gère lorsque les nuages ne sont pas échantillonnés pareil dans la prochaine partie (erreur plus élevée, NN plus compliqué). 
\subsection{Etude de l'influence de $d_{max}$}
\label{part:influencedmax}
%Explication de d_max
La mise en correspondance des points des deux nuages est une étape cruciale de l'algorithme ICP. La solution la plus répandue consiste à apparier les points par recherche du plus proche voisin. Cependant les nuages ne se recoupent pas complètement ainsi certains points d'un nuage ne corresponde à aucun point de l'autre nuage (occlusion, recouvrement partiel). Pour prendre cela en compte, une solution est de retirer les points appariés dont la distance est supérieure à un seuil $d_{max}$.\\

%TODO Etude théorique : +dmax petit, +précis mais moins de chance de convergence et inversement
En général, le choix de $d_{max}$ est un compromis entre la précision de l'algorithme et sa portée de convergence. En effet, une valeur faible de $d_{max}$ entraîne un faible nombre de points appariés rendant la convergence de l'algorithme plus difficile. Au contraire, une valeur plus élevée de $d_{max}$ peut causer des appariements inexactes et ainsi éloigner l'estimation de la transformation de sa valeur exacte.\\

%Présentation des résultats
Afin d'évaluer l'influence de $d_{max}$ sur la performance de la méthode proposée, l'erreur de position moyenne a été mesuré sur les résultats obtenus par l'algorithme standard de l'ICP et par GICP pour différentes valeurs de $d_{max}$ sur les jeux de donnée  synthétique et réel.\\


\begin{figure}[!h]
   \centering
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/bunny_evol_error_dmax.jpg}
     \caption{Données synthétiques}
     \label{fig:err_dmax_synthe}
   \end{subfigure}%
   ~
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/hannover_evol_error_dmax.jpg}
     \caption{Données réelles}
     \label{fig:err_dmax_reelle}
   \end{subfigure}
   
   \caption{Evolution de l'erreur de position moyenne en fonction de la valeur du seuil $d_{max}$.}
   \label{fig:err_dmax}
\end{figure}



\begin{figure}[!h]
   \centering
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/bunny_size_subset_dmax.jpg}
     \caption{Données synthétiques}
     \label{fig:subset_dmax_synthe}
   \end{subfigure}%
   ~
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/hannover_size_subset_dmax.jpg}
     \caption{Données réelles}
     \label{fig:subset_dmax_reelle}
   \end{subfigure}
   
   \caption{Evolution du nombre de points appariés lors de la première itération de l'algorithme en fonction de la valeur du seuil $d_{max}$.}
   \label{fig:subset_dmax}
\end{figure}

Tout d'abord, on peut constater que pour une valeur trop faible de $d_{max}$, l'algorithme ne converge ni sur les données synthétiques, ni sur les données réelles car l'ensemble des points appariés est trop faible pour permettre à l'algorithme de converger (comme le montre le nombre de points appariés lors de la première itération de l'algorithme pour une valeur faible de $d_{max}$ sur la figure \ref{fig:subset_dmax}).\\

L'absence d'occlusion, l'échantillonnage identique, et l'absence d'outliers entre les deux nuages synthétiques entrainent une influence nulle de $d_{max}$ sur la performance du recalage sur les données synthétiques. En effet, les nuages de point parviennent toujours à se recaler (figure \ref{fig:err_dmax_synthe}).\\

Au contraire, sur les données réelles, on peut voir que l'erreur moyenne se dégrade pour l'ICP standard lorsque $d_{max}$ dépasse une certaine valeur, $d_{max}=200$ tandis que l'influence sur GICP reste très faible. En effet, de nombreux appariements sont inexactes lorsque la valeur de $d_{max}$ est élevée, ce qui à pour conséquence d'éloigner le résultat de la solution exacte. Pour GICP, ces appariements erronés contribue cependant très peu à la fonction de coût car les plans locaux à ces points ne sont pas forcément coplanaires (voir la figure \ref{fig_cov}). Ainsi, comme le montre la figure \ref{fig:err_dmax_reelle}, l'erreur moyenne est stable lorsque la valeur de $d_{max}$ augmente pour GICP tandis qu'elle se dégrade pour l'ICP standard.\\

De plus on peut observer que l'erreur moyenne obtenue par GICP est inférieure ou égale à celle obtenue par ICP standard, cela tend à mettre en évidence une meilleure performance de GICP par rapport à l'ICP Standard.\\

\begin{figure}[!h]
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/hannover_dmax200_diffconvergence.jpg}
     \caption{Comparaison de la vitesse de convergence sur les données réelles pour $d_{max}=200$ entre GICP et ICP standard.}
	\label{fig:GICP_convergence}
\end{figure}

Enfin, comme le montre la figure \ref{fig:GICP_convergence}, on constate que GICP converge beaucoup plus rapidement que l'ICP Standard. L'image \ref{fig:meshlab} montre la consolidation des nuages de points de la base Hannovre après recalage pour $d_{max} = 200$.




\begin{figure}[H]
   \centering
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.5]{Images/Resultats/hannover_meshlab_before_transformation.jpg}
     \caption{Nuages avant recalage}
   \end{subfigure}%
   ~
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.5]{Images/Resultats/hannover_meshlab_after_transformation.jpg}
     \caption{Nuages après recalage}
   \end{subfigure}
   
   \caption{Consolidation des nuages de points Hannovre avec GICP ($d_{max} = 200$).}
   \label{fig:meshlab}
\end{figure}
   
\subsection{Étude de l'influence de l'offset}

Pour évaluer les différents algorithmes, Segal et al. utilisent une transformation initiale $\mathbf{T}_0$ qui est définie comme la transformation exacte $\mathbf{T}^*$ à laquelle est ajoutée une erreur entre $\pm1.5m$ et $\pm15^{\circ}$ selon tous les axes. \\

L'idée ici est de comparer GICP et l'ICP standard pour des valeurs de $T_0$ qui s'éloignent de $T^*$ afin d'étudier la robustesse des deux algorithmes. Pour cela, nous avons fixé la matrice $\mathbf{T}_0=\mathbf{T}^*+\epsilon $ où $\epsilon$ est un terme d'erreur constant compris entre $\pm0.1cm$ et $\pm15^{\circ}$ selon tous les axes puis nous avons fait varier l'erreur de la rotation $\theta$ autour de l'axe des $x$ entre $[-20^{\circ} -65^{\circ}]$.\\

\begin{figure}[!h]
   \centering
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/bunny_offset_evol-error_sicp.jpg}
     \caption{Standard ICP}
   \end{subfigure}%
   ~
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/bunny_offset_evol-error_gicp.jpg}
     \caption{Generalized ICP}
   \end{subfigure}
   
   \caption{Évolution de l'erreur de position moyenne en fonction de la rotation $\theta$ selon l'axe des $x$ de $\mathbf{T}_0$ sur des données synthétiques.}
   \label{fig:offset}
\end{figure}


La figure \ref{fig:offset} montre l'évolution de l'erreur moyenne de position obtenue par GICP et ICP standard en fonction de la valeur de l'angle $\theta_{x}$. On constate qu'à partir d'une certaine valeur ($\theta = -60^{\circ}$), GICP ne converge plus. L'ICP standard continue à converger sur les valeurs de $\theta_{x}$ que nous avons testé.\\

Ainsi, la robustesse de la méthode Generalized ICP à l'offset initial $\mathbf{T}_0$ semble moins bonne que pour l'ICP standard. Il est donc important, lors de l'utilisation de GICP, d'avoir une transformation initiale $\mathbf{T}_0$ pas trop éloignée de la transformation exacte $\mathbf{T}^*$.

\subsection{Standard ICP par solution analytique}

Comme présenté dans la partie \ref{part:impl_gicp}, le modèle probabiliste introduit par Segal permet d'implémenter différentes variantes de l'ICP en modifiant judicieusement les matrices de covariance $C_{i}^A$ et $C_{i}^B$. Ainsi, il est intéressant de comparer les résultats obtenus avec la version de l'ICP standard modélisé dans le framework GICP et les résultats obtenus avec la formule analytique de l'ICP standard. \\

Pour cela, nous avons testé l'ICP standard par formule analytique pour les mêmes valeurs de $d_{max}$ que dans la partie \ref{part:influencedmax}. Les résultats sont présentés dans la figure \ref{fig:closedform_ICP}. On peut constater que l'erreur moyenne de position obtenu est pratiquement identique entre les deux méthodes sur la figure \ref{fig:CFICP:err}. Ainsi le modèle probabiliste présenté permet bien de retrouver l'ICP standard en posant  $C_{i}^A = 0$ et $C_{i}^B = Id$.\\

\begin{figure}[!h]
   \centering
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/hannover_diff_ICP_closed_form_dmax.jpg}
     \caption{Erreur moyenne}
     \label{fig:CFICP:err}
   \end{subfigure}%
   ~
   \begin{subfigure}[t]{.5\linewidth}
     \centering
     \includegraphics[scale=0.4]{Images/Resultats/hannover_diff_time_ICP_closed_form_dmax.jpg}
     \caption{Temps d'exécution}
     \label{fig:CFICP:time}
   \end{subfigure}
   
   \caption{Comparaison des résultats obtenus avec la formule analytique de l'ICP standard (en rouge) et avec le framework de GICP (en bleu).}
   \label{fig:closedform_ICP}
\end{figure}

En revanche, comme le montre la figure \ref{fig:CFICP:time}, le temps d'exécution est beaucoup plus long avec le framework GICP. Cette forte différence s'explique par l'utilisation, dans notre implémentation, d'une fonction de minimisation par Quasi-newton tandis que la minimisation est explicite dans la formule analytique de l'ICP. La méthode de minimisation par descente de gradient conjugué dont il est fait mention dans \cite{bib_gicp} est sans doute plus rapide. 